{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1874598,"sourceType":"datasetVersion","datasetId":1115942},{"sourceId":6704311,"sourceType":"datasetVersion","datasetId":3863975},{"sourceId":7432092,"sourceType":"datasetVersion","datasetId":4324996},{"sourceId":10616273,"sourceType":"datasetVersion","datasetId":6572802},{"sourceId":10625764,"sourceType":"datasetVersion","datasetId":6579000},{"sourceId":10626852,"sourceType":"datasetVersion","datasetId":6579642},{"sourceId":10648098,"sourceType":"datasetVersion","datasetId":6593107},{"sourceId":10684666,"sourceType":"datasetVersion","datasetId":6619548},{"sourceId":10685521,"sourceType":"datasetVersion","datasetId":6620194},{"sourceId":225046,"sourceType":"modelInstanceVersion","modelInstanceId":191963,"modelId":213920},{"sourceId":249182,"sourceType":"modelInstanceVersion","modelInstanceId":212994,"modelId":234631}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom torchvision.models import resnet18\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:55:33.447341Z","iopub.execute_input":"2025-02-11T16:55:33.447637Z","iopub.status.idle":"2025-02-11T16:55:37.955505Z","shell.execute_reply.started":"2025-02-11T16:55:33.447611Z","shell.execute_reply":"2025-02-11T16:55:37.95474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\n\n# Dataset path\ndatapath = '/kaggle/input/trash-type-image-dataset/TrashType_Image_Dataset'\n\n# Define transformations for training and validation\ntarget_size = (224, 224)\nbatch_size = 16\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=10),  # Mild rotation\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Mild color variation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for pre-trained models\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize(target_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load training and validation datasets\ntrain_dataset = datasets.ImageFolder(root=datapath, transform=train_transforms)\nval_size = int(0.1 * len(train_dataset))\ntrain_size = len(train_dataset) - val_size\n\ntrain_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\nprint(f\"Training samples: {len(train_data)}\")\nprint(f\"Validation samples: {len(val_data)}\")\n\n# Show some random images from the training set\ndef show_images(dataloader, class_names):\n    images, labels = next(iter(dataloader))\n    plt.figure(figsize=(10, 10))\n    for i in range(min(batch_size, 9)):\n        plt.subplot(3, 3, i + 1)\n        img = images[i].permute(1, 2, 0) * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n        plt.imshow(img.clamp(0, 1))\n        plt.title(class_names[labels[i].item()])\n        plt.axis('off')\n    plt.show()\n\n# Class names from the dataset\nclass_names = train_dataset.classes\nprint(f\"Class names: {class_names}\")\nshow_images(train_loader, class_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T13:21:31.992276Z","iopub.execute_input":"2025-02-10T13:21:31.992837Z","iopub.status.idle":"2025-02-10T13:21:40.998352Z","shell.execute_reply.started":"2025-02-10T13:21:31.992791Z","shell.execute_reply":"2025-02-10T13:21:40.997175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Path to the RealWaste dataset\nrealwaste_dataset_path = \"/kaggle/input/realwaste/realwaste-main/RealWaste\"\n\n# Function to plot sample images from a dataset\ndef plot_sample_images(dataset_path, num_samples=3):\n    plt.figure(figsize=(15, 15))\n    \n    # Get all class directories from the dataset\n    class_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n\n    for idx, class_dir in enumerate(class_dirs):\n        class_path = os.path.join(dataset_path, class_dir)\n        images = os.listdir(class_path)\n        random_samples = random.sample(images, min(num_samples, len(images)))\n\n        # Plot the images for each class\n        for i, image_name in enumerate(random_samples):\n            img_path = os.path.join(class_path, image_name)\n            img = plt.imread(img_path)\n            plt.subplot(len(class_dirs), num_samples, idx * num_samples + i + 1)\n            plt.imshow(img)\n            plt.axis(\"off\")\n            plt.title(class_dir)\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot 3 random images from each class in the RealWaste dataset\nplot_sample_images(realwaste_dataset_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:07:30.994599Z","iopub.execute_input":"2025-02-10T04:07:30.994845Z","iopub.status.idle":"2025-02-10T04:07:34.143796Z","shell.execute_reply.started":"2025-02-10T04:07:30.994823Z","shell.execute_reply":"2025-02-10T04:07:34.142852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:57:15.010062Z","iopub.execute_input":"2025-02-11T16:57:15.010362Z","iopub.status.idle":"2025-02-11T16:57:15.429743Z","shell.execute_reply.started":"2025-02-11T16:57:15.010341Z","shell.execute_reply":"2025-02-11T16:57:15.428575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Define dataset paths\ntrash_dataset = \"/kaggle/input/trash-type-image-dataset/TrashType_Image_Dataset\"\nreal_dataset = \"/kaggle/input/realwaste/realwaste-main/RealWaste\"\n\n# Define output paths\ntrash_test_path = \"/kaggle/working/trash_test\"\nreal_test_path = \"/kaggle/working/real_test\"\n\n# Ensure test directories exist\nos.makedirs(trash_test_path, exist_ok=True)\nos.makedirs(real_test_path, exist_ok=True)\n\n# Function to copy test images\ndef split_data(src_folder, dest_folder, test_ratio=0.1):\n    for class_name in os.listdir(src_folder):\n        class_path = os.path.join(src_folder, class_name)\n        if os.path.isdir(class_path):\n            images = os.listdir(class_path)\n            test_count = int(len(images) * test_ratio)\n            test_images = random.sample(images, test_count)\n\n            class_test_path = os.path.join(dest_folder, class_name)\n            os.makedirs(class_test_path, exist_ok=True)\n\n            for img in test_images:\n                shutil.copy(os.path.join(class_path, img), os.path.join(class_test_path, img))\n\n# Split both datasets\nsplit_data(trash_dataset, trash_test_path)\nsplit_data(real_dataset, real_test_path)\n\nprint(\"Testing sets copied successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:57:24.153222Z","iopub.execute_input":"2025-02-11T16:57:24.15355Z","iopub.status.idle":"2025-02-11T16:57:25.489927Z","shell.execute_reply.started":"2025-02-11T16:57:24.153523Z","shell.execute_reply":"2025-02-11T16:57:25.489023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom PIL import Image\n\n# Define dataset paths\ntrash_dataset = \"/kaggle/input/trash-type-image-dataset/TrashType_Image_Dataset\"\nreal_dataset = \"/kaggle/input/realwaste/realwaste-main/RealWaste\"\noutput_dir = \"/kaggle/working/combined_dataset\"\n\n# Define class mappings\nclass_mapping = {\n    \"Cardboard\": [\"cardboard\", \"Cardboard\"],\n    \"Glass\": [\"glass\", \"Glass\"],\n    \"Metal\": [\"metal\", \"Metal\"],\n    \"Paper\": [\"paper\", \"Paper\"],\n    \"Plastic\": [\"plastic\", \"Plastic\"],\n    \"Miscellaneous Trash\": [\"trash\", \"Miscellaneous Trash\"],\n    \"Organics\": [\"Food Organics\", \"Vegetation\"],\n    \"Textile Trash\": [\"Textile Trash\"]\n}\n\n# Ensure output directories exist\nos.makedirs(output_dir, exist_ok=True)\nfor class_name in class_mapping.keys():\n    os.makedirs(os.path.join(output_dir, class_name), exist_ok=True)\n\n# Function to copy images and count class distribution\nclass_counts = Counter()\n\ndef copy_images(src_folder, class_names, dest_folder):\n    for class_name in class_names:\n        class_path = os.path.join(src_folder, class_name)\n        if os.path.isdir(class_path):\n            images = os.listdir(class_path)\n            class_counts[dest_folder] += len(images)\n            for img in images:\n                src_img_path = os.path.join(class_path, img)\n                dest_img_path = os.path.join(dest_folder, img)\n                shutil.copy(src_img_path, dest_img_path)\n\n# Copy images from both datasets into the new structure\nfor new_class, old_classes in class_mapping.items():\n    dest_folder = os.path.join(output_dir, new_class)\n    copy_images(trash_dataset, old_classes, dest_folder)\n    copy_images(real_dataset, old_classes, dest_folder)\n\n# Plot sample images\ndef plot_sample_images():\n    fig, axes = plt.subplots(len(class_mapping), 5, figsize=(15, 20))\n    for i, (class_name, _) in enumerate(class_mapping.items()):\n        class_folder = os.path.join(output_dir, class_name)\n        images = os.listdir(class_folder)\n        sample_images = random.sample(images, min(5, len(images)))  # Take up to 5 images\n\n        for j, img_name in enumerate(sample_images):\n            img_path = os.path.join(class_folder, img_name)\n            img = Image.open(img_path)\n            axes[i, j].imshow(img)\n            axes[i, j].axis(\"off\")\n            if j == 0:\n                axes[i, j].set_title(class_name, fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot class imbalance chart\ndef plot_class_distribution():\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Number of Images\")\n    plt.title(\"Class Distribution in Combined Dataset\")\n    plt.show()\n\n# Show plots\nplot_sample_images()\nplot_class_distribution()\n\nprint(\"New dataset with merged classes created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:57:27.231688Z","iopub.execute_input":"2025-02-11T16:57:27.232008Z","iopub.status.idle":"2025-02-11T16:57:43.870278Z","shell.execute_reply.started":"2025-02-11T16:57:27.231984Z","shell.execute_reply":"2025-02-11T16:57:43.869364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Define source folders\nclothes_folder = \"/kaggle/input/garbage-classification/garbage_classification/clothes\"\nshoes_folder = \"/kaggle/input/garbage-classification/garbage_classification/shoes\"\n\n# Define destination folders\ntextile_trash_folder = \"/kaggle/working/combined_dataset/Textile Trash\"\n\n# Ensure destination folders exist\nos.makedirs(textile_trash_folder, exist_ok=True)\n\n# Function to copy a subset of images\ndef copy_images(src_folder, dest_folder, num_images):\n    images = os.listdir(src_folder)\n    selected_images = random.sample(images, min(num_images, len(images)))  # Ensure we don't exceed available images\n    \n    for img in selected_images:\n        src_path = os.path.join(src_folder, img)\n        dest_path = os.path.join(dest_folder, img)\n        shutil.copy(src_path, dest_path)\n\n# Add 400 images from clothes to Textile Trash\ncopy_images(clothes_folder, textile_trash_folder, 400)\n\n# Add 200 images from shoes: classify them based on material if possible\ncopy_images(shoes_folder, textile_trash_folder, 200)  # Assuming most are fabric/leather\n# If material separation is needed, manual sorting might be required\n\nprint(\"Added 400 images from clothes and 200 images from shoes to the dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:57:49.029147Z","iopub.execute_input":"2025-02-11T16:57:49.029454Z","iopub.status.idle":"2025-02-11T16:57:54.028635Z","shell.execute_reply.started":"2025-02-11T16:57:49.029428Z","shell.execute_reply":"2025-02-11T16:57:54.027685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path=\"/kaggle/working/combined_dataset\"\n# List subdirectories (classes)\nclasses = os.listdir(dataset_path)\nprint(f\"Classes: {classes}\")\n\n# Count the number of images in each class\nfor cls in classes:\n    cls_path = os.path.join(dataset_path, cls)\n    num_images = len(os.listdir(cls_path))\n    print(f\"{cls}: {num_images} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:58:00.097924Z","iopub.execute_input":"2025-02-11T16:58:00.098196Z","iopub.status.idle":"2025-02-11T16:58:00.108853Z","shell.execute_reply.started":"2025-02-11T16:58:00.098175Z","shell.execute_reply":"2025-02-11T16:58:00.108113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Paths for the source and destination datasets\nsource_dataset_path = \"/kaggle/input/data-cus/custom_test\"  # Source folder with images\ndestination_dataset_path = \"/kaggle/working/newcombined1\"  # Destination folder to save combined dataset\n\n# Ensure the destination folder exists\nos.makedirs(destination_dataset_path, exist_ok=True)\n\n# List the classes (folders) from the source dataset\nclasses = os.listdir(source_dataset_path)\n\n# Iterate over each class and copy images to the destination dataset\nfor class_name in classes:\n    class_source_path = os.path.join(source_dataset_path, class_name)\n    class_dest_path = os.path.join(destination_dataset_path, class_name)\n    \n    if os.path.isdir(class_source_path):\n        # Create the class folder in the destination directory if it doesn't exist\n        os.makedirs(class_dest_path, exist_ok=True)\n        \n        # Get the list of image files in the class folder\n        images = os.listdir(class_source_path)\n        \n        # Copy each image to the corresponding folder in the destination dataset\n        for img_name in images:\n            img_source_path = os.path.join(class_source_path, img_name)\n            img_dest_path = os.path.join(class_dest_path, img_name)\n            \n            # Copy the image\n            shutil.copy(img_source_path, img_dest_path)\n\n        print(f\"Copied {len(images)} images to {class_name} class.\")\n\nprint(\"Image copying completed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:58:20.577144Z","iopub.execute_input":"2025-02-11T16:58:20.577414Z","iopub.status.idle":"2025-02-11T16:58:22.072478Z","shell.execute_reply.started":"2025-02-11T16:58:20.577393Z","shell.execute_reply":"2025-02-11T16:58:22.071784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path=\"/kaggle/working/newcombined1\"\n# List subdirectories (classes)\nclasses = os.listdir(dataset_path)\nprint(f\"Classes: {classes}\")\n\n# Count the number of images in each class\nfor cls in classes:\n    cls_path = os.path.join(dataset_path, cls)\n    num_images = len(os.listdir(cls_path))\n    print(f\"{cls}: {num_images} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T13:23:10.542235Z","iopub.execute_input":"2025-02-10T13:23:10.542559Z","iopub.status.idle":"2025-02-10T13:23:10.558285Z","shell.execute_reply.started":"2025-02-10T13:23:10.542529Z","shell.execute_reply":"2025-02-10T13:23:10.557457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport matplotlib.pyplot as plt\n\n# Path to the merged dataset (Maindataset)\nmaindataset_path = \"/kaggle/working/newcombined\"\n\n# Function to plot sample images from a dataset\ndef plot_sample_images(dataset_path, num_samples=3):\n    plt.figure(figsize=(15, 15))\n    \n    # Get all class directories from the dataset\n    class_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n\n    for idx, class_dir in enumerate(class_dirs):\n        class_path = os.path.join(dataset_path, class_dir)\n        images = os.listdir(class_path)\n        random_samples = random.sample(images, min(num_samples, len(images)))\n\n        # Plot the images for each class\n        for i, image_name in enumerate(random_samples):\n            img_path = os.path.join(class_path, image_name)\n            img = plt.imread(img_path)\n            plt.subplot(len(class_dirs), num_samples, idx * num_samples + i + 1)\n            plt.imshow(img)\n            plt.axis(\"off\")\n            plt.title(class_dir)\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot 3 random images from each class in the Maindataset\nplot_sample_images(maindataset_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:13:36.449168Z","iopub.execute_input":"2025-02-09T13:13:36.449548Z","iopub.status.idle":"2025-02-09T13:13:39.06358Z","shell.execute_reply.started":"2025-02-09T13:13:36.449518Z","shell.execute_reply":"2025-02-09T13:13:39.06267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Count the number of images in each class\ndef count_class_images(dataset_path):\n    class_counts = {}\n    class_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n\n    for class_dir in class_dirs:\n        class_path = os.path.join(dataset_path, class_dir)\n        images = os.listdir(class_path)\n        class_counts[class_dir] = len(images)\n\n    return class_counts\n\n# Plot class distribution\ndef plot_class_distribution(class_counts):\n    classes = list(class_counts.keys())\n    counts = list(class_counts.values())\n\n    plt.figure(figsize=(10, 6))\n    plt.barh(classes, counts, color='skyblue')\n    plt.xlabel('Number of Images')\n    plt.title('Class Distribution')\n    plt.show()\n\n# Get class counts\nclass_counts = count_class_images(maindataset_path)\n\n# Print the class counts\nfor class_name, count in class_counts.items():\n    print(f\"Class: {class_name}, Count: {count}\")\n\n# Plot the class distribution\nplot_class_distribution(class_counts)\n\n# Check for class imbalance\ntotal_images = sum(class_counts.values())\nimbalanced_classes = {cls: count for cls, count in class_counts.items() if count < total_images / len(class_counts)}\n\nif imbalanced_classes:\n    print(\"\\nClasses with fewer images than average (potentially imbalanced):\")\n    for class_name, count in imbalanced_classes.items():\n        print(f\"{class_name}: {count} images\")\nelse:\n    print(\"\\nNo class imbalance detected.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T06:10:58.661309Z","iopub.execute_input":"2025-02-07T06:10:58.661622Z","iopub.status.idle":"2025-02-07T06:10:58.90947Z","shell.execute_reply.started":"2025-02-07T06:10:58.661595Z","shell.execute_reply":"2025-02-07T06:10:58.908799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Path to the directory to be zipped\ndirectory_to_zip = \"/kaggle/working/combined_dataset\"\n# Path where the zip file will be saved\nzip_file_path = \"/kaggle/working/combined_dataset.zip\"\n\n# Create a zip file of the directory\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', directory_to_zip)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:59:03.531219Z","iopub.execute_input":"2025-02-11T16:59:03.531498Z","iopub.status.idle":"2025-02-11T16:59:03.535527Z","shell.execute_reply.started":"2025-02-11T16:59:03.531478Z","shell.execute_reply":"2025-02-11T16:59:03.534672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = \"/kaggle/working/newcombined1\"  \noutput_dir = \"/kaggle/working/waste-split\"  \n\ntrain_ratio = 0.7\nval_ratio = 0.2\ntest_ratio = 0.1\n\nfor split in ['train', 'val', 'test']:\n    os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n\nfor class_name in os.listdir(data_dir):\n    class_dir = os.path.join(data_dir, class_name)\n    if not os.path.isdir(class_dir):\n        continue  \n\n    images = [img for img in os.listdir(class_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]\n\n    if len(images) == 0:\n        print(f\"Warning: No images found for class '{class_name}'. Skipping.\")\n        continue\n\n    train_images, temp_images = train_test_split(images, test_size=(val_ratio + test_ratio), random_state=42)\n    val_images, test_images = train_test_split(temp_images, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n\n    for split, split_images in zip(['train', 'val', 'test'], [train_images, val_images, test_images]):\n        split_class_dir = os.path.join(output_dir, split, class_name)\n        os.makedirs(split_class_dir, exist_ok=True)\n\n        for img in split_images:\n            src_path = os.path.join(class_dir, img)\n            dest_path = os.path.join(split_class_dir, img)\n            shutil.copy(src_path, dest_path)\n\nprint(f\"Dataset split complete. Splits saved in: {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:59:08.679866Z","iopub.execute_input":"2025-02-11T16:59:08.680159Z","iopub.status.idle":"2025-02-11T16:59:10.017255Z","shell.execute_reply.started":"2025-02-11T16:59:08.680137Z","shell.execute_reply":"2025-02-11T16:59:10.016402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install umap-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T07:25:50.6965Z","iopub.execute_input":"2025-02-11T07:25:50.696889Z","iopub.status.idle":"2025-02-11T07:25:54.188789Z","shell.execute_reply.started":"2025-02-11T07:25:50.696857Z","shell.execute_reply":"2025-02-11T07:25:54.187727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import umap.umap_ as umap\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# Load test dataset\ntest_dataset = datasets.ImageFolder('/kaggle/working/waste-split/test', transform=transform_val)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n\nfeatures = []\nlabels_list = []\n\n# Get the class names from the dataset\nclass_names = test_dataset.classes\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        # Extract features before the fully connected layers\n        x = model.conv_block1(images)\n        x = model.conv_block2(x)\n        x = model.conv_block3(x)\n        x = model.conv_block4(x)\n        embeddings = model.global_avg_pool(x)  # Use the global average pooling layer\n        embeddings = torch.flatten(embeddings, 1)\n\n        features.append(embeddings.cpu().numpy())\n        labels_list.extend(labels.cpu().numpy())\n\nfeatures = np.vstack(features)\nlabels_list = np.array(labels_list)\n\n# UMAP Visualization with distinct colors for each class\nreducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')\numap_embeddings = reducer.fit_transform(features)\n\n# Define a color map with distinct colors for each class\ncmap = ListedColormap(plt.cm.get_cmap(\"tab10\").colors[:len(np.unique(labels_list))])\n\n# Create the UMAP plot\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=labels_list, cmap=cmap, alpha=0.6)\n\n# Create a legend for class names\nhandles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(i), markersize=10) for i in range(len(class_names))]\nplt.legend(handles, class_names, title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Remove ticks and title\nplt.xticks([])\nplt.yticks([])\nplt.title(\"UMAP Visualization of Feature Embeddings\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T07:44:26.316067Z","iopub.execute_input":"2025-02-11T07:44:26.316419Z","iopub.status.idle":"2025-02-11T07:44:36.905543Z","shell.execute_reply.started":"2025-02-11T07:44:26.316391Z","shell.execute_reply":"2025-02-11T07:44:36.904448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport os\nfrom tqdm import tqdm\nimport numpy as np\n\n# ------------------------------- Data Preparation ------------------------------- #\n# Define transformations for training and validation datasets\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), shear=10),\n    transforms.GaussianBlur(kernel_size=3),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntransform_val = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load datasets\ntrain_dataset = datasets.ImageFolder('/kaggle/working/waste-split/train', transform=transform_train)\nval_dataset = datasets.ImageFolder('/kaggle/working/waste-split/val', transform=transform_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# ------------------------------- Compute Class Weights ------------------------------- #\n# Get the class counts from the dataset\nclass_counts = np.bincount(train_dataset.targets)\ntotal_samples = len(train_dataset)\nclass_weights = total_samples / class_counts  # Inverse of frequency\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).cuda()  # Move to GPU if available\n\n# ------------------------------- Load Pre-trained Model ------------------------------- #\n# Load ResNet50 with pre-trained weights\nmodel = models.resnet50(weights='IMAGENET1K_V1')\n\n# Modify the last fully connected layer to match the number of classes in your dataset\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 8)  # Assuming 8 classes in your dataset\n\n# ------------------------------- Step 1: Freeze All Layers ------------------------------- #\n# Freeze all convolutional layers, leaving FC layers trainable\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the FC layers\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\n# Define the optimizer to only update FC layers\noptimizer = optim.Adam(model.fc.parameters(), lr=1e-4)\n\n# Weighted Cross-Entropy Loss\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# ------------------------------- Step 2: Train Only FC Layer ------------------------------- #\ndef train_fc(model, train_loader, val_loader, optimizer, criterion, num_epochs=5):\n    model.train()\n    best_val_accuracy = 0.0  # To track the best validation accuracy\n    best_model_path = '/kaggle/working//best_fine_tuned_teacher_model.pth'  # Path to save the best model\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        # Use tqdm for progress bar\n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", unit='batch'):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        # Training loss and accuracy\n        train_loss = running_loss / len(train_loader)\n        train_accuracy = 100 * correct / total\n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        \n        # Validation\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=\"Validation\", unit='batch'):\n                inputs, labels = inputs.cuda(), labels.cuda()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        # Validation loss and accuracy\n        val_loss = val_loss / len(val_loader)\n        val_accuracy = 100 * correct / total\n        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n\n        # Save the model if validation accuracy improves\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"Saved best model with Validation Accuracy: {val_accuracy:.2f}%\")\n\n        model.train()\n\n# Move model to GPU if available\nmodel = model.cuda()\n\n# Train the model (FC layer only)\ntrain_fc(model, train_loader, val_loader, optimizer, criterion, num_epochs=5)\n\n# ------------------------------- Step 3: Unfreeze Last 20 Layers ------------------------------- #\n# Unfreeze the last 20 layers of the model\ndef unfreeze_last_layers(model, num_layers=20):\n    # Get all the layers of the model\n    layers = list(model.children())\n    \n    # Unfreeze the last 'num_layers' layers\n    for layer in layers[-num_layers:]:\n        for param in layer.parameters():\n            param.requires_grad = True\n    print(f\"Unfroze the last {num_layers} layers.\")\n\n# Unfreeze last 20 layers and fine-tune\nunfreeze_last_layers(model, num_layers=20)\n\n# Define optimizer again (now update both FC layers and the last layers)\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\n\n# ------------------------------- Step 4: Fine-tune the Model ------------------------------- #\ndef fine_tune(model, train_loader, val_loader, optimizer, criterion, num_epochs=5):\n    model.train()\n    best_val_accuracy = 0.0  # To track the best validation accuracy\n    best_model_path = '/kaggle/working//best_fine_tuned_teacher_model.pth'  # Path to save the best model\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        # Use tqdm for progress bar\n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Fine-tuning\", unit='batch'):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        # Training loss and accuracy\n        train_loss = running_loss / len(train_loader)\n        train_accuracy = 100 * correct / total\n        print(f\"Fine-tune Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        \n        # Validation\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=\"Validation\", unit='batch'):\n                inputs, labels = inputs.cuda(), labels.cuda()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        # Validation loss and accuracy\n        val_loss = val_loss / len(val_loader)\n        val_accuracy = 100 * correct / total\n        print(f\"Fine-tune Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n\n        # Save the model if validation accuracy improves\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"Saved best fine-tuned model with Validation Accuracy: {val_accuracy:.2f}%\")\n        \n        model.train()\n\n# Fine-tune the model\nfine_tune(model, train_loader, val_loader, optimizer, criterion, num_epochs=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T17:17:45.214428Z","iopub.execute_input":"2025-02-11T17:17:45.214743Z","iopub.status.idle":"2025-02-11T17:34:35.970155Z","shell.execute_reply.started":"2025-02-11T17:17:45.214719Z","shell.execute_reply":"2025-02-11T17:34:35.969203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport numpy as np\n\n# ------------------------------- CNN Model (Student) ------------------------------- #\nclass CNNModel(nn.Module):\n    def __init__(self, num_classes=8):\n        super(CNNModel, self).__init__()\n\n        self.conv_block1 = self._create_conv_block(3, 64)\n        self.conv_block2 = self._create_conv_block(64, 128)\n        self.conv_block3 = self._create_conv_block(128, 256)\n        self.conv_block4 = self._create_conv_block(256, 512)\n        self.conv_block5 = self._create_conv_block(512, 512)\n\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Linear(512, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        self.dropout = nn.Dropout(0.3)\n\n    def _create_conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.conv_block3(x)\n        x = self.conv_block4(x)\n        x = self.conv_block5(x)\n        x = self.global_avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = nn.functional.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# ------------------------------- Knowledge Distillation Loss ------------------------------- #\ndef distillation_loss(student_outputs, teacher_outputs, temperature=2.0):\n    \"\"\"\n    Calculate the distillation loss as the Kullback-Leibler divergence between the student and teacher's soft targets.\n    \"\"\"\n    teacher_probs = torch.nn.functional.softmax(teacher_outputs / temperature, dim=1)\n    student_probs = torch.nn.functional.log_softmax(student_outputs / temperature, dim=1)\n    loss = torch.nn.functional.kl_div(student_probs, teacher_probs, reduction='batchmean')\n    return loss\n\n# ------------------------------- Load Pretrained Teacher Model ------------------------------- #\nteacher_model = models.resnet50(weights='IMAGENET1K_V1')\nnum_ftrs = teacher_model.fc.in_features\nteacher_model.fc = nn.Linear(num_ftrs, 8)  # Adjust to match your dataset's class count\n\n# Load the saved teacher model's weights (make sure to save the teacher model after fine-tuning it)\nteacher_model.load_state_dict(torch.load('/kaggle/working//best_fine_tuned_teacher_model.pth'))\nteacher_model = teacher_model.cuda()  # Move to GPU if available\nteacher_model.eval()  # Set the teacher model to evaluation mode\n\n# ------------------------------- Train Student Model with Knowledge Distillation ------------------------------- #\ndef train_student_model(student_model, teacher_model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=10, temperature=2.0):\n    student_model.train()\n    best_val_accuracy = 0.0\n    best_model_path = '/kaggle/working//best_fine_tuned_teacher_model.pth'\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", unit='batch'):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n            optimizer.zero_grad()\n\n            # Get the teacher's soft targets\n            with torch.no_grad():\n                teacher_outputs = teacher_model(inputs)\n\n            # Get the student model's predictions\n            student_outputs = student_model(inputs)\n\n            # Compute distillation loss and cross-entropy loss\n            distill_loss = distillation_loss(student_outputs, teacher_outputs, temperature)\n            ce_loss = criterion(student_outputs, labels)\n\n            # Combine both losses\n            total_loss = ce_loss + distill_loss\n\n            total_loss.backward()\n            optimizer.step()\n\n            running_loss += total_loss.item()\n\n            # Compute accuracy\n            _, predicted = torch.max(student_outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        # Training loss and accuracy\n        train_loss = running_loss / len(train_loader)\n        train_accuracy = 100 * correct / total\n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n\n        # Validation\n        student_model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=\"Validation\", unit='batch'):\n                inputs, labels = inputs.cuda(), labels.cuda()\n                student_outputs = student_model(inputs)\n                loss = criterion(student_outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(student_outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        # Validation loss and accuracy\n        val_loss = val_loss / len(val_loader)\n        val_accuracy = 100 * correct / total\n        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n\n        # Save the model if validation accuracy improves\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            torch.save(student_model.state_dict(), best_model_path)\n            print(f\"Saved best student model with Validation Accuracy: {val_accuracy:.2f}%\")\n\n        # Step the learning rate scheduler based on validation loss\n        scheduler.step(val_loss)\n\n        student_model.train()\n\n# ------------------------------- Set Up Optimizer, Loss, and Scheduler ------------------------------- #\n# Define optimizer and criterion for student model\nstudent_model = CNNModel(num_classes=8).cuda()\noptimizer = optim.Adam(student_model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n# Define learning rate scheduler (ReduceLROnPlateau)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n\n# ------------------------------- Start Training with Knowledge Distillation ------------------------------- #\ntrain_student_model(student_model, teacher_model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T17:35:52.876871Z","iopub.execute_input":"2025-02-11T17:35:52.877189Z","iopub.status.idle":"2025-02-11T17:49:13.361067Z","shell.execute_reply.started":"2025-02-11T17:35:52.877163Z","shell.execute_reply":"2025-02-11T17:49:13.360038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_student_model(student_model, teacher_model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=15)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T17:49:13.362423Z","iopub.execute_input":"2025-02-11T17:49:13.362777Z","iopub.status.idle":"2025-02-11T18:30:13.03133Z","shell.execute_reply.started":"2025-02-11T17:49:13.362751Z","shell.execute_reply":"2025-02-11T18:30:13.030336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_student_model(student_model, teacher_model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T18:30:13.032735Z","iopub.execute_input":"2025-02-11T18:30:13.033017Z","iopub.status.idle":"2025-02-11T19:24:13.741066Z","shell.execute_reply.started":"2025-02-11T18:30:13.032996Z","shell.execute_reply":"2025-02-11T19:24:13.740103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = CNNModel(num_classes=8)\nmodel.load_state_dict(torch.load('/kaggle/working/best_fine_tuned_teacher_model.pth'))\nmodel.eval()  # Set the model to evaluation mode before inference\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T19:27:02.357695Z","iopub.execute_input":"2025-02-11T19:27:02.358046Z","iopub.status.idle":"2025-02-11T19:27:02.484506Z","shell.execute_reply.started":"2025-02-11T19:27:02.358022Z","shell.execute_reply":"2025-02-11T19:27:02.483343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# ------------------------------- Function: Compute Per-Class Accuracy ------------------------------- #\ndef evaluate_model(model, data_loader, device, num_classes):\n    model.eval()\n    correct_per_class = torch.zeros(num_classes).to(device)\n    total_per_class = torch.zeros(num_classes).to(device)\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n            for i in range(num_classes):\n                correct_per_class[i] += (preds[labels == i] == i).sum().item()\n                total_per_class[i] += (labels == i).sum().item()\n\n    per_class_acc = correct_per_class / (total_per_class + 1e-6)  # Avoid division by zero\n    print(\"\\nðŸš€ **Per-Class Accuracy on Test Set:**\")\n    for i in range(num_classes):\n        print(f\"Class {i}: {per_class_acc[i].item()*100:.2f}%\")\n    \n    return all_preds, all_labels\n\n# ------------------------------- Function: Plot Confusion Matrix ------------------------------- #\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n# ------------------------------- Load Test Data ------------------------------- #\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntest_dataset = datasets.ImageFolder('/kaggle/working/waste-split/test', transform=transform_test)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# ------------------------------- Model Evaluation on Test Set ------------------------------- #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Class Names for Confusion Matrix\nclass_names = [\"Cardboard\", \"Misc. Trash\", \"Glass\", \"Organics\", \"Metal\", \"Plastic\", \"Textile Trash\", \"Paper\"]\n\n# Run Evaluation\ny_pred, y_true = evaluate_model(model, test_loader, device, num_classes=8)\n\n# Plot Confusion Matrix\nplot_confusion_matrix(y_true, y_pred, class_names)\n\n# Print Detailed Classification Report\nprint(\"\\n **Classification Report on Test Set:**\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T19:24:44.016386Z","iopub.execute_input":"2025-02-11T19:24:44.016739Z","iopub.status.idle":"2025-02-11T19:24:51.090251Z","shell.execute_reply.started":"2025-02-11T19:24:44.016711Z","shell.execute_reply":"2025-02-11T19:24:51.089492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# ------------------------------- Function: Compute Per-Class Accuracy ------------------------------- #\ndef evaluate_model(model, data_loader, device, num_classes):\n    model.eval()\n    correct_per_class = torch.zeros(num_classes).to(device)\n    total_per_class = torch.zeros(num_classes).to(device)\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n            for i in range(num_classes):\n                correct_per_class[i] += (preds[labels == i] == i).sum().item()\n                total_per_class[i] += (labels == i).sum().item()\n\n    per_class_acc = correct_per_class / (total_per_class + 1e-6)  # Avoid division by zero\n    print(\"\\nðŸš€ **Per-Class Accuracy on Test Set:**\")\n    for i in range(num_classes):\n        print(f\"Class {i}: {per_class_acc[i].item()*100:.2f}%\")\n    \n    return all_preds, all_labels\n\n# ------------------------------- Function: Plot Confusion Matrix ------------------------------- #\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n# ------------------------------- Load Test Data ------------------------------- #\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntest_dataset = datasets.ImageFolder('/kaggle/working/custom', transform=transform_test)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# ------------------------------- Model Evaluation on Test Set ------------------------------- #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Class Names for Confusion Matrix\nclass_names = [\"Cardboard\", \"Misc. Trash\", \"Glass\", \"Organics\", \"Metal\", \"Plastic\", \"Textile Trash\", \"Paper\"]\n\n# Run Evaluation\ny_pred, y_true = evaluate_model(model, test_loader, device, num_classes=8)\n\n# Plot Confusion Matrix\nplot_confusion_matrix(y_true, y_pred, class_names)\n\n# Print Detailed Classification Report\nprint(\"\\nðŸ“Š **Classification Report on Test Set:**\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T19:25:24.378241Z","iopub.execute_input":"2025-02-11T19:25:24.378723Z","iopub.status.idle":"2025-02-11T19:25:25.93949Z","shell.execute_reply.started":"2025-02-11T19:25:24.378676Z","shell.execute_reply":"2025-02-11T19:25:25.93879Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resnet50 ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import transforms, datasets\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import lr_scheduler\nfrom torchvision.models import resnet50\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Data Augmentation and Preprocessing\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Simple Transform for Validation and Test (No augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Custom dataset class\nclass CustomDataset(datasets.ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root, transform)\n\n# Merge RealWaste and Custom Datasets\nrealwaste_data = CustomDataset(root='/kaggle/input/realwaste/realwaste-main/RealWaste', transform=train_transform)\ncustom_data = CustomDataset(root='/kaggle/input/custom-data/custom_test', transform=train_transform)\n\n# Combine datasets (RealWaste + Custom)\ncombined_dataset = ConcatDataset([realwaste_data, custom_data])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T04:56:57.829624Z","iopub.execute_input":"2025-02-05T04:56:57.829955Z","iopub.status.idle":"2025-02-05T04:57:02.347917Z","shell.execute_reply.started":"2025-02-05T04:56:57.829931Z","shell.execute_reply":"2025-02-05T04:57:02.346956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Split into Train, Validation, and Test\ntrain_size = int(0.7 * len(combined_dataset))\nval_size = int(0.2 * len(combined_dataset))\ntest_size = len(combined_dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(combined_dataset, [train_size, val_size, test_size])\n\n# Apply validation transform (no augmentation) on validation and test datasets\nval_dataset.dataset.transform = val_transform\ntest_dataset.dataset.transform = val_transform\n\n# DataLoader for training, validation, and test sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Class Weights Calculation\ntarget_labels = []\n\n# Iterate over individual datasets in combined dataset to collect labels\nfor dataset in [realwaste_data, custom_data]:\n    target_labels.extend(dataset.targets)  # `targets` contains the class labels in ImageFolder\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(target_labels), y=target_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).cuda()  # Moving weights to GPU if available\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T04:57:02.349032Z","iopub.execute_input":"2025-02-05T04:57:02.349272Z","iopub.status.idle":"2025-02-05T04:57:02.359549Z","shell.execute_reply.started":"2025-02-05T04:57:02.349251Z","shell.execute_reply":"2025-02-05T04:57:02.358598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2., alpha=0.25, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        BCE_loss = nn.CrossEntropyLoss(weight=class_weights, reduction='none')(inputs, targets)\n        targets = targets.view(-1, 1)\n        P = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1 - P) ** self.gamma * BCE_loss\n        if self.reduction == 'mean':\n            return F_loss.mean()\n        elif self.reduction == 'sum':\n            return F_loss.sum()\n        else:\n            return F_loss\n\n\n# Load Pretrained ResNet50 and Modify for Fine-tuning\nmodel = resnet50(pretrained=True)\n\n# Replace the last fully connected layer\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(np.unique(target_labels)))  # Number of classes in your dataset\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Loss Function: Weighted Cross-Entropy\ncriterion = FocalLoss(gamma=2., alpha=0.25).to(device)\n\n# Optimizer and Learning Rate Scheduler\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n# Fine-tuning with Validation\nnum_epochs = 10\nbest_val_accuracy = 0.0\npatience = 5  # Early stopping patience\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    # Training loop\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    scheduler.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    val_accuracy = 100 * val_correct / val_total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%, Val Accuracy: {val_accuracy:.2f}%\")\n\n    # Check if this is the best model so far\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        counter = 0  # Reset patience counter\n        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break\n\n# Load the best model for testing\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Testing the model\nmodel.eval()\ntest_correct = 0\ntest_total = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        test_total += labels.size(0)\n        test_correct += (predicted == labels).sum().item()\n\ntest_accuracy = 100 * test_correct / test_total\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:53:44.698744Z","iopub.execute_input":"2025-02-04T09:53:44.699053Z","iopub.status.idle":"2025-02-04T10:01:12.726592Z","shell.execute_reply.started":"2025-02-04T09:53:44.699019Z","shell.execute_reply":"2025-02-04T10:01:12.725481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install shap\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T16:35:16.125837Z","iopub.execute_input":"2025-02-04T16:35:16.126336Z","iopub.status.idle":"2025-02-04T16:35:20.842006Z","shell.execute_reply.started":"2025-02-04T16:35:16.126299Z","shell.execute_reply":"2025-02-04T16:35:20.84071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import transforms, datasets\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import lr_scheduler\nfrom torchvision.models import resnet50\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2., alpha=0.25, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        BCE_loss = nn.CrossEntropyLoss(weight=class_weights, reduction='none')(inputs, targets)\n        targets = targets.view(-1, 1)\n        P = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1 - P) ** self.gamma * BCE_loss\n        if self.reduction == 'mean':\n            return F_loss.mean()\n        elif self.reduction == 'sum':\n            return F_loss.sum()\n        else:\n            return F_loss\n\n\n# Data Augmentation and Preprocessing\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Simple Transform for Validation and Test (No augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Custom dataset class\nclass CustomDataset(datasets.ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root, transform)\n\n# Merge RealWaste and Custom Datasets\nrealwaste_data = CustomDataset(root='/kaggle/input/realwaste/realwaste-main/RealWaste', transform=train_transform)\ncustom_data = CustomDataset(root='/kaggle/input/custom-data/custom_test', transform=train_transform)\n\n# Combine datasets (RealWaste + Custom)\ncombined_dataset = ConcatDataset([realwaste_data, custom_data])\n\n# Split into Train, Validation, and Test\ntrain_size = int(0.7 * len(combined_dataset))\nval_size = int(0.2 * len(combined_dataset))\ntest_size = len(combined_dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(combined_dataset, [train_size, val_size, test_size])\n\n# Apply validation transform (no augmentation) on validation and test datasets\nval_dataset.dataset.transform = val_transform\ntest_dataset.dataset.transform = val_transform\n\n# DataLoader for training, validation, and test sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Class Weights Calculation\ntarget_labels = []\n\n# Iterate over individual datasets in combined dataset to collect labels\nfor dataset in [realwaste_data, custom_data]:\n    target_labels.extend(dataset.targets)  # `targets` contains the class labels in ImageFolder\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(target_labels), y=target_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).cuda()  # Moving weights to GPU if available\n\n# Load Pretrained ResNet50 and Modify for Fine-tuning\nmodel = resnet50(pretrained=True)\n\n# Replace the last fully connected layer\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(np.unique(target_labels)))  # Number of classes in your dataset\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Loss Function: Weighted Cross-Entropy\ncriterion = FocalLoss(gamma=2., alpha=0.25).to(device)\n\n# Optimizer and Learning Rate Scheduler\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n# Fine-tuning with Validation\nnum_epochs = 10\nbest_val_accuracy = 0.0\npatience = 5  # Early stopping patience\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    # Training loop\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    scheduler.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    val_accuracy = 100 * val_correct / val_total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%, Val Accuracy: {val_accuracy:.2f}%\")\n\n    # Check if this is the best model so far\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        counter = 0  # Reset patience counter\n        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break\n\n# Load the best model for testing\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Testing the model\nmodel.eval()\ntest_correct = 0\ntest_total = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        test_total += labels.size(0)\n        test_correct += (predicted == labels).sum().item()\n\ntest_accuracy = 100 * test_correct / test_total\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T16:41:22.041208Z","iopub.execute_input":"2025-02-04T16:41:22.041532Z","iopub.status.idle":"2025-02-04T16:49:06.536414Z","shell.execute_reply.started":"2025-02-04T16:41:22.0415Z","shell.execute_reply":"2025-02-04T16:49:06.535349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test Transform (same as training transform without augmentation)\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Load the custom test dataset\ncustom_test_data = datasets.ImageFolder(root='/kaggle/input/test-last/test', transform=test_transform)\n\n# DataLoader for test set\ntest_loader = DataLoader(custom_test_data, batch_size=32, shuffle=False, num_workers=4)\n# Load the best model saved during training\nmodel.load_state_dict(torch.load('/kaggle/input/best_model/pytorch/default/1/best_model (1).pth'))\nmodel.eval()\n\n# Testing the model with custom test data\ncorrect = 0\ntotal = 0\nclass_correct = [0] * len(custom_test_data.classes)\nclass_total = [0] * len(custom_test_data.classes)\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        # Track class-wise accuracy\n        for i in range(labels.size(0)):\n            label = labels[i]\n            class_correct[label] += (predicted[i] == label).item()\n            class_total[label] += 1\n\n# Overall accuracy\naccuracy = 100 * correct / total\nprint(f\"Overall Test Accuracy on Custom Data: {accuracy:.2f}%\")\n\n# Class-wise accuracy\nfor i in range(len(custom_test_data.classes)):\n    if class_total[i] > 0:\n        class_accuracy = 100 * class_correct[i] / class_total[i]\n        print(f\"Accuracy for class {custom_test_data.classes[i]}: {class_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T10:04:15.422878Z","iopub.execute_input":"2025-02-04T10:04:15.423191Z","iopub.status.idle":"2025-02-04T10:04:17.317434Z","shell.execute_reply.started":"2025-02-04T10:04:15.423164Z","shell.execute_reply":"2025-02-04T10:04:17.315759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport shap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom torchvision import models, datasets\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\n\n# Paths\nmodel_path = \"/kaggle/input/best_model/pytorch/default/1/best_model (1).pth\"\ndataset_path = \"/kaggle/input/realwaste/realwaste-main/RealWaste/test\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the ResNet-50 model\nmodel = models.resnet50(pretrained=False)\nmodel.fc = torch.nn.Linear(model.fc.in_features, 9)  # 9 classes\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval().to(device)\n\n# # Define transformations\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n# # Load test dataset\n# test_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Class names for visualization\nclass_names = ['Metal', 'Glass', 'Paper', 'Vegetation', \n               'Cardboard', 'Textile Trash', 'Food Organics', \n               'Plastic', 'Miscellaneous Trash']\n\n# Function to get one incorrect image per predicted class\ndef get_incorrect_images_per_class(model, test_loader):\n    incorrect_images_per_class = defaultdict(lambda: None)  # Store one image per incorrect predicted class\n    incorrect_labels_per_class = {}\n    incorrect_preds_per_class = {}\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n\n            # Find incorrect predictions\n            for img, label, pred in zip(images, labels, preds):\n                if pred != label and pred.item() not in incorrect_images_per_class:\n                    incorrect_images_per_class[pred.item()] = img.cpu()\n                    incorrect_labels_per_class[pred.item()] = label.cpu()\n                    incorrect_preds_per_class[pred.item()] = pred.cpu()\n    \n    return incorrect_images_per_class, incorrect_labels_per_class, incorrect_preds_per_class\n\n# Fetch one incorrect image per class\nincorrect_images, incorrect_labels, incorrect_preds = get_incorrect_images_per_class(model, test_loader)\n\n# SHAP Visualization for one image per incorrect predicted class\ndef plot_shap_per_incorrect_class(model, incorrect_images, incorrect_labels, incorrect_preds, class_names):\n    # Use SHAP Gradient Explainer with a random image as background\n    background = torch.stack(list(incorrect_images.values())[:4]).to(device)  # Use up to 4 images for background\n    explainer = shap.GradientExplainer(model, background)\n\n    # Plot for each incorrect class prediction\n    fig, axes = plt.subplots(len(incorrect_images), len(class_names), figsize=(20, len(incorrect_images) * 4))\n    \n    for i, (pred_class, img) in enumerate(incorrect_images.items()):\n        test_image = img.unsqueeze(0).to(device)\n        shap_values = explainer.shap_values(test_image)\n\n        for j in range(len(class_names)):\n            ax = axes[i, j] if len(incorrect_images) > 1 else axes[j]\n            shap_contrib = np.abs(shap_values[j][0]).sum(axis=0)\n\n            img_np = test_image.cpu().numpy().transpose(0, 2, 3, 1)[0]\n            ax.imshow(img_np)\n            ax.imshow(shap_contrib, cmap='hot', alpha=0.5)\n            true_label = class_names[incorrect_labels[pred_class].item()]\n            pred_label = class_names[incorrect_preds[pred_class].item()]\n            ax.axis('off')\n            ax.set_title(f\"({class_names[j]}\\nTrue: {true_label}\\nPred: {pred_label}\")\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function if there are any incorrect predictions\nif incorrect_images:\n    plot_shap_per_incorrect_class(model, incorrect_images, incorrect_labels, incorrect_preds, class_names)\nelse:\n    print(\"No incorrect predictions found!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T04:59:16.521016Z","iopub.execute_input":"2025-02-05T04:59:16.521335Z","iopub.status.idle":"2025-02-05T05:01:23.657182Z","shell.execute_reply.started":"2025-02-05T04:59:16.521314Z","shell.execute_reply":"2025-02-05T05:01:23.655655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Initialize lists to store true labels and predictions\nall_labels = []\nall_predictions = []\n\n# Evaluate on custom test data\nmodel.eval()\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        \n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\n# Calculate confusion matrix\ncm = confusion_matrix(all_labels, all_predictions)\n\n# Display the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=custom_test_data.classes)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix for Custom Test Data\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T10:04:22.85579Z","iopub.execute_input":"2025-02-04T10:04:22.856121Z","iopub.status.idle":"2025-02-04T10:04:24.106954Z","shell.execute_reply.started":"2025-02-04T10:04:22.856087Z","shell.execute_reply":"2025-02-04T10:04:24.105997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport shap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom torchvision import models, datasets\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\nimport os\n\n# Paths for custom test data\nmodel_path = \"/kaggle/input/best_model/pytorch/default/1/best_model (1).pth\"\ncustom_test_dataset_path = \"/kaggle/input/test-last/test\"  # Directory where your custom test data is located\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the ResNet-50 model\nmodel = models.resnet50(pretrained=False)\nmodel.fc = torch.nn.Linear(model.fc.in_features, 9)  # 9 classes\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.to(device)  # Ensure the model is moved to the correct device\nmodel.eval()\n\n# Define transformations for custom test data\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load custom test dataset\ncustom_test_dataset = datasets.ImageFolder(root=custom_test_dataset_path, transform=transform)\ncustom_test_loader = DataLoader(custom_test_dataset, batch_size=32, shuffle=False)\n\n# Class names for visualization\nclass_names = ['Metal', 'Glass', 'Paper', 'Vegetation', \n               'Cardboard', 'Textile Trash', 'Food Organics', \n               'Plastic', 'Miscellaneous Trash']\n\n# Function to get one incorrect image per predicted class from custom test data\ndef get_incorrect_images_per_class(model, test_loader):\n    incorrect_images_per_class = defaultdict(lambda: None)  # Store one image per incorrect predicted class\n    incorrect_labels_per_class = {}\n    incorrect_preds_per_class = {}\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)  # Move inputs to the same device as the model\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n\n            # Find incorrect predictions\n            for img, label, pred in zip(images, labels, preds):\n                if pred != label and pred.item() not in incorrect_images_per_class:\n                    incorrect_images_per_class[pred.item()] = img.cpu()\n                    incorrect_labels_per_class[pred.item()] = label.cpu()\n                    incorrect_preds_per_class[pred.item()] = pred.cpu()\n    \n    return incorrect_images_per_class, incorrect_labels_per_class, incorrect_preds_per_class\n\n# Fetch one incorrect image per class from custom test data\nincorrect_images, incorrect_labels, incorrect_preds = get_incorrect_images_per_class(model, custom_test_loader)\n\n# SHAP Visualization for one image per incorrect predicted class from custom test data\ndef plot_shap_per_incorrect_class(model, incorrect_images, incorrect_labels, incorrect_preds, class_names):\n    # Ensure background images are on the same device\n    background = torch.stack(list(incorrect_images.values())[:4]).to(device)  # Use up to 4 images for background\n    explainer = shap.GradientExplainer(model, background)\n\n    # Plot for each incorrect class prediction\n    fig, axes = plt.subplots(len(incorrect_images), len(class_names), figsize=(20, len(incorrect_images) * 4))\n    \n    for i, (pred_class, img) in enumerate(incorrect_images.items()):\n        test_image = img.unsqueeze(0).to(device)  # Ensure test image is on the same device\n        shap_values = explainer.shap_values(test_image)\n\n        for j in range(len(class_names)):\n            ax = axes[i, j] if len(incorrect_images) > 1 else axes[j]\n            shap_contrib = np.abs(shap_values[j][0]).sum(axis=0)\n\n            img_np = test_image.cpu().numpy().transpose(0, 2, 3, 1)[0]\n            ax.imshow(img_np)\n            ax.imshow(shap_contrib, cmap='hot', alpha=0.5)\n            true_label = class_names[incorrect_labels[pred_class].item()]\n            pred_label = class_names[incorrect_preds[pred_class].item()]\n            ax.axis('off')\n            ax.set_title(f\"({class_names[j]})\\nTrue: {true_label}\\nPred: {pred_label} \")\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function if there are any incorrect predictions\nif incorrect_images:\n    plot_shap_per_incorrect_class(model, incorrect_images, incorrect_labels, incorrect_preds, class_names)\nelse:\n    print(\"No incorrect predictions found!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T05:03:54.377849Z","iopub.execute_input":"2025-02-05T05:03:54.378293Z","iopub.status.idle":"2025-02-05T05:05:05.289072Z","shell.execute_reply.started":"2025-02-05T05:03:54.378258Z","shell.execute_reply":"2025-02-05T05:05:05.288022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\n\n# Test Transform (same as training transform without augmentation)\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Load the custom test dataset\ncustom_test_data = datasets.ImageFolder(root='/kaggle/input/test-last/test', transform=test_transform)\n\n# DataLoader for test set\ntest_loader = DataLoader(custom_test_data, batch_size=32, shuffle=False, num_workers=4)\n\n# Load the best model saved during training\nmodel.load_state_dict(torch.load('/kaggle/input/best_model/pytorch/default/1/best_model (1).pth'))\nmodel.eval()\n\n# ImageNet mean and std for unnormalization\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\n# Testing the model with custom test data\ncorrect = 0\ntotal = 0\nclass_correct = [0] * len(custom_test_data.classes)\nclass_total = [0] * len(custom_test_data.classes)\n\n# Get the class names from the dataset\nclass_names = custom_test_data.classes\n\n# List to store images and labels for later plotting\nimages_to_show = []\ntrue_labels = []\npredicted_labels = []\n\n# Start testing\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        # Track class-wise accuracy\n        for i in range(labels.size(0)):\n            label = labels[i]\n            class_correct[label] += (predicted[i] == label).item()\n            class_total[label] += 1\n\n        # Collect images and corresponding true and predicted labels\n        for i in range(inputs.size(0)):\n            img = inputs[i].cpu().numpy().transpose((1, 2, 0))  # (C, H, W) to (H, W, C)\n            img = std * img + mean  # Unnormalize using ImageNet stats\n            img = np.clip(img, 0, 1)  # Ensure pixel values are between 0 and 1\n            images_to_show.append(img)\n            true_labels.append(class_names[labels[i]])\n            predicted_labels.append(class_names[predicted[i]])\n\n# Overall accuracy\naccuracy = 100 * correct / total\nprint(f\"Overall Test Accuracy on Custom Data: {accuracy:.2f}%\")\n\n# Class-wise accuracy\nfor i in range(len(custom_test_data.classes)):\n    if class_total[i] > 0:\n        class_accuracy = 100 * class_correct[i] / class_total[i]\n        print(f\"Accuracy for class {custom_test_data.classes[i]}: {class_accuracy:.2f}%\")\n\n# Now display all the images sequentially with their true and predicted labels\nplt.figure(figsize=(15, 15))\nfor i in range(len(images_to_show)):\n    plt.subplot(8, 8, i + 1)\n    plt.imshow(images_to_show[i])\n    plt.title(f\"True: {true_labels[i]}\\nPred: {predicted_labels[i]}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T10:04:30.4657Z","iopub.execute_input":"2025-02-04T10:04:30.466083Z","iopub.status.idle":"2025-02-04T10:04:34.284567Z","shell.execute_reply.started":"2025-02-04T10:04:30.466056Z","shell.execute_reply":"2025-02-04T10:04:34.283173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom CNN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import transforms, datasets\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import lr_scheduler\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# CBAM: Convolutional Block Attention Module\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction=16, kernel_size=7):\n        super(CBAM, self).__init__()\n        # Channel attention\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n            nn.ReLU(),\n            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n        )\n        self.sigmoid_channel = nn.Sigmoid()\n        # Spatial attention\n        self.spatial = nn.Sequential(\n            nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Channel attention\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        channel_att = self.sigmoid_channel(avg_out + max_out)\n        x = x * channel_att\n\n        # Spatial attention\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out,_ = torch.max(x, dim=1, keepdim=True)\n        spatial_att = self.spatial(torch.cat([avg_out, max_out], dim=1))\n        x = x * spatial_att\n        return x\n\n# Bottleneck Residual Block with CBAM\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None, reduction=16):\n        super(BottleneckBlock, self).__init__()\n        mid_channels = out_channels // 4\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        self.cbam = CBAM(out_channels, reduction=reduction)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out = self.cbam(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n            \n        out += identity\n        out = self.relu(out)\n        return out\n\n# Advanced CNN using multiple Bottleneck Blocks\nclass AdvancedCNN(nn.Module):\n    def __init__(self, num_classes=9):\n        super(AdvancedCNN, self).__init__()\n        # Initial stem\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),  # 224 -> 112\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1)  # 112 -> 56\n        )\n        # Create layers (similar to ResNet-50 style)\n        self.layer1 = self._make_layer(64, 256, blocks=3, stride=1)   # 56x56\n        self.layer2 = self._make_layer(256, 512, blocks=4, stride=2)    # 56 -> 28\n        self.layer3 = self._make_layer(512, 1024, blocks=6, stride=2)   # 28 -> 14\n        self.layer4 = self._make_layer(1024, 2048, blocks=3, stride=2)  # 14 -> 7\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(2048, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, blocks, stride):\n        downsample = None\n        if stride != 1 or in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        layers = []\n        layers.append(BottleneckBlock(in_channels, out_channels, stride, downsample))\n        for _ in range(1, blocks):\n            layers.append(BottleneckBlock(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.layer1(x)  # output channels: 256\n        x = self.layer2(x)  # 512\n        x = self.layer3(x)  # 1024\n        x = self.layer4(x)  # 2048\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T09:39:33.794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ========================\n# Data Augmentation and Dataset Setup\n# ========================\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225])\n])\n\n# A custom dataset class (uses ImageFolder)\nclass CustomDataset(datasets.ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root, transform)\n\n# Replace the paths below with your dataset directories\nrealwaste_data = CustomDataset(root='/kaggle/input/realwaste/realwaste-main/RealWaste', transform=train_transform)\ncustom_data = CustomDataset(root='/kaggle/input/custom-data/custom_test', transform=train_transform)\n\n# Combine datasets\ncombined_dataset = ConcatDataset([realwaste_data, custom_data])\n\n# Split dataset: 70% train, 20% validation, 10% test (or adjust as needed)\ntotal_size = len(combined_dataset)\ntrain_size = int(0.7 * total_size)\nval_size = int(0.2 * total_size)\ntest_size = total_size - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(combined_dataset, [train_size, val_size, test_size])\n\n# Use the non-augmented transform for validation and test sets\nval_dataset.dataset.transform = val_transform\ntest_dataset.dataset.transform = val_transform\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n\n# ========================\n# Class Weights Calculation\n# ========================\n# Collect all targets from each individual dataset\ntarget_labels = []\nfor dataset in [realwaste_data, custom_data]:\n    target_labels.extend(dataset.targets)  # ImageFolder stores labels in .targets\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(target_labels), y=target_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T09:39:33.794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================\n# Model, Loss, Optimizer, Scheduler Setup\n# ========================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AdvancedCNN(num_classes=len(np.unique(target_labels)))\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n\n# ========================\n# Training Loop with Early Stopping\n# ========================\nnum_epochs = 50\nbest_val_accuracy = 0.0\npatience = 5\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    scheduler.step()\n    \n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n    \n    train_acc = 100 * correct / total\n    val_acc = 100 * val_correct / val_total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, \"\n          f\"Train Acc: {train_acc:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%\")\n    \n    # Early stopping check\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        counter = 0\n        torch.save(model.state_dict(), 'best_model_custom.pth')\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break\n\n# ========================\n# Testing the Best Model\n# ========================\nmodel.load_state_dict(torch.load('best_model_custom.pth'))\nmodel.eval()\ntest_correct = 0\ntest_total = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        test_total += labels.size(0)\n        test_correct += (predicted == labels).sum().item()\n\ntest_accuracy = 100 * test_correct / test_total\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T09:39:33.794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the previously saved model weights\nmodel.load_state_dict(torch.load('best_model_custom.pth'))\n\n# Set the model back to training mode\nmodel.train()\n\n# Continuing from where we left off, we will train for 15 more epochs\nnum_additional_epochs = 20\nstarting_epoch = 30 # Start counting from the next epoch\n\nfor epoch in range(starting_epoch, starting_epoch + num_additional_epochs):\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    scheduler.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    train_acc = 100 * correct / total\n    val_acc = 100 * val_correct / val_total\n    print(f\"Epoch [{epoch+1}/{starting_epoch + num_additional_epochs}], \"\n          f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n          f\"Train Acc: {train_acc:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, \"\n          f\"Val Acc: {val_acc:.2f}%\")\n\n    # Early stopping check\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        torch.save(model.state_dict(), 'best_model_custom.pth')\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T09:39:33.794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Initialize lists to store true labels and predictions\nall_labels = []\nall_predictions = []\n\n# Evaluate on custom test data\nmodel.eval()\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        \n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\n# Calculate confusion matrix\ncm = confusion_matrix(all_labels, all_predictions)\n\n# Display the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=custom_test_data.classes)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix for Custom Test Data\")\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T09:39:33.794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load custom test data from a separate folder\ntest_custom_dataset = datasets.ImageFolder(root='/kaggle/input/test-last/test', transform=val_transform)\ntest_custom_loader = DataLoader(test_custom_dataset, batch_size=32, shuffle=False, num_workers=4)\n\nall_preds = []\nall_labels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for inputs, labels in test_custom_loader:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Compute confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(10,8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix on Custom Test Data\")\nplt.show()\n\n# Print classification report\nprint(classification_report(all_labels, all_preds))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T09:39:33.794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import transforms, datasets\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import lr_scheduler\nimport numpy as np\n\n# Squeeze-and-Excitation Block\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n# Residual Block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, use_se=True):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.se = SELayer(out_channels) if use_se else nn.Identity()\n        self.shortcut = nn.Sequential()\n\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.se(out)\n        out += self.shortcut(x)\n        return F.relu(out)\n\n# Enhanced CNN Model\nclass EnhancedCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(EnhancedCNN, self).__init__()\n        self.layer1 = ResidualBlock(3, 64)\n        self.layer2 = ResidualBlock(64, 128, stride=2)\n        self.layer3 = ResidualBlock(128, 256, stride=2)\n        self.layer4 = ResidualBlock(256, 512, stride=2)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512, 1024),\n            nn.Dropout(0.5),\n            nn.ReLU(),\n            nn.Linear(1024, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.global_pool(x)\n        return self.fc(x)\n\n# Data Augmentation and Preprocessing\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Simple Transform for Validation and Test (No augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Custom dataset class\nclass CustomDataset(datasets.ImageFolder):\n    def __init__(self, root, transform=None):\n        super(CustomDataset, self).__init__(root, transform)\n\n# Merge RealWaste and Custom Datasets\nrealwaste_data = CustomDataset(root='/kaggle/input/realwaste/realwaste-main/RealWaste', transform=train_transform)\ncustom_data = CustomDataset(root='/kaggle/input/custom-data/custom_test', transform=train_transform)\n\n# Combine datasets (RealWaste + Custom)\ncombined_dataset = ConcatDataset([realwaste_data, custom_data])\n\n# Split into Train, Validation, and Test\ntrain_size = int(0.7 * len(combined_dataset))\nval_size = int(0.2 * len(combined_dataset))\ntest_size = len(combined_dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(combined_dataset, [train_size, val_size, test_size])\n\n# Apply validation transform (no augmentation) on validation and test datasets\nval_dataset.dataset.transform = val_transform\ntest_dataset.dataset.transform = val_transform\n\n# DataLoader for training, validation, and test sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Class Weights Calculation\ntarget_labels = []\n\n# Iterate over individual datasets in combined dataset to collect labels\nfor dataset in [realwaste_data, custom_data]:\n    target_labels.extend(dataset.targets)  # `targets` contains the class labels in ImageFolder\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(target_labels), y=target_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).cuda()  # Moving weights to GPU if available\n\n# Load EnhancedCNN Model and Modify for Fine-tuning\nmodel = EnhancedCNN(num_classes=len(np.unique(target_labels)))\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Loss Function: Weighted Cross-Entropy\ncriterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n\n# Optimizer and Learning Rate Scheduler\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n# Fine-tuning with Validation\nnum_epochs = 50\nbest_val_accuracy = 0.0\npatience = 5  # Early stopping patience\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    # Training loop\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    scheduler.step()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    val_accuracy = 100 * val_correct / val_total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%, Val Accuracy: {val_accuracy:.2f}%\")\n\n    # Check if this is the best model so far\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        counter = 0  # Reset patience counter\n        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break\n\n# Load the best model for testing\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Testing the model\nmodel.eval()\ntest_correct = 0\ntest_total = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        test_total += labels.size(0)\n        test_correct += (predicted == labels).sum().item()\n\ntest_accuracy = 100 * test_correct / test_total\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T09:39:33.794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split, ConcatDataset\nfrom torchvision import transforms, datasets\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim import lr_scheduler\nimport numpy as np\nfrom torchvision.datasets import ImageFolder\nfrom fastai.vision.all import *\n\n# Custom dataset class with augmentation\nclass CustomDataset(datasets.ImageFolder):\n    def __init__(self, root, transform=None, augment=False):\n        super(CustomDataset, self).__init__(root, transform)\n        self.augment = augment\n        self.original_data = self.samples  # Store original images to later augment\n\n    def __getitem__(self, index):\n        # Get the original image\n        img, label = self.samples[index]\n        img = self.loader(img)\n        \n        if self.augment:\n            # Apply augmentations (5 augmentations per image)\n            augmented_images = [self.transform(img) for _ in range(5)]\n            augmented_images.append(img)  # Adding the original image\n            augmented_labels = [label] * 6  # 6 images per sample (original + 5 augments)\n            return augmented_images, augmented_labels\n        else:\n            return self.transform(img), label\n\n# Squeeze-and-Excitation Block\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n# Residual Block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, use_se=True):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.se = SELayer(out_channels) if use_se else nn.Identity()\n        self.shortcut = nn.Sequential()\n\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.se(out)\n        out += self.shortcut(x)\n        return F.relu(out)\n\n# Enhanced CNN Model\nclass EnhancedCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(EnhancedCNN, self).__init__()\n        self.layer1 = ResidualBlock(3, 64)\n        self.layer2 = ResidualBlock(64, 128, stride=2)\n        self.layer3 = ResidualBlock(128, 256, stride=2)\n        self.layer4 = ResidualBlock(256, 512, stride=2)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512, 1024),\n            nn.Dropout(0.5),\n            nn.ReLU(),\n            nn.Linear(1024, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.global_pool(x)\n        return self.fc(x)\n\n# Data Augmentation and Preprocessing\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Simple Transform for Validation and Test (No augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.0, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load RealWaste and Custom Dataset\nrealwaste_data = CustomDataset(root='/kaggle/input/realwaste/realwaste-main/RealWaste', transform=train_transform)\ncustom_data = CustomDataset(root='/kaggle/input/custom-data/custom_test', transform=train_transform, augment=True)\n\nfrom collections import Counter\n\n# Function to count the images per class in a dataset\ndef count_images_per_class(dataset):\n    # Returns a Counter dictionary of class indices and their counts\n    return Counter([sample[1] for sample in dataset.samples])\n\n# Print count of images per class before augmentation\nprint(\"Before Augmentation:\")\nprint(\"RealWaste Data (Balanced to 600 images per class):\")\nbalanced_realwaste_data_counts = count_images_per_class(balanced_realwaste_data)\nfor class_idx, count in balanced_realwaste_data_counts.items():\n    print(f\"Class {realwaste_data.classes[class_idx]}: {count} images\")\n\n\n# Resample RealWaste Data to 600 images per class (downsampling or oversampling)\nbalanced_realwaste_data = []\nfor class_idx in range(len(realwaste_data.classes)):\n    class_samples = [sample for sample in realwaste_data.samples if sample[1] == class_idx]\n    if len(class_samples) > 600:\n        balanced_realwaste_data += class_samples[:600]  # Downsample to 600 images per class\n    else:\n        balanced_realwaste_data += class_samples * (600 // len(class_samples))  # Oversample to 600 images per class\n\n# Combine balanced RealWaste data with augmented Custom data\ncombined_data = ConcatDataset([ImageFolder(root='/kaggle/input/realwaste/realwaste-main/RealWaste', transform=train_transform), custom_data])\n\n# Split combined dataset into train, validation, and test\ntrain_size = int(0.7 * len(combined_data))\nval_size = int(0.2 * len(combined_data))\ntest_size = len(combined_data) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(combined_data, [train_size, val_size, test_size])\n\n# Apply validation transform (no augmentation) on validation and test datasets\nval_dataset.dataset.transform = val_transform\ntest_dataset.dataset.transform = val_transform\n\n# DataLoader for training, validation, and test sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Class Weights Calculation\ntarget_labels = []\nfor dataset in [realwaste_data, custom_data]:\n    target_labels.extend(dataset.targets)\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(target_labels), y=target_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).cuda()\n\n# Initialize EnhancedCNN Model\nmodel = EnhancedCNN(num_classes=len(realwaste_data.classes))\n\n# Move model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Loss Function: Weighted Cross-Entropy\ncriterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n\n# Optimizer and Learning Rate Scheduler\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n# Learning Rate Finder using FastAI\nlearn = cnn_learner(Datasets(train_loader, val_loader), EnhancedCNN, metrics=accuracy, lr_find=True)\n\n# Train model using optimal learning rate from the learning rate finder\nlearn.fit_one_cycle(10, lr_max=1e-4)\n\n# Test model accuracy\nlearn.validate(test_loader)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T09:39:33.794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n\nimport shap\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:01:01.181265Z","iopub.execute_input":"2025-02-04T17:01:01.181579Z","iopub.status.idle":"2025-02-04T17:01:09.38881Z","shell.execute_reply.started":"2025-02-04T17:01:01.181557Z","shell.execute_reply":"2025-02-04T17:01:09.387761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load pre-trained model and data\nmodel = ResNet50(weights=\"imagenet\")\nX, y = shap.datasets.imagenet50()\n\n# getting ImageNet 1000 class names\nurl = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\nwith open(shap.datasets.cache(url)) as file:\n    class_names = [v[1] for v in json.load(file).values()]\n# print(\"Number of ImageNet classes:\", len(class_names))\n# print(\"Class names:\", class_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:01:09.390195Z","iopub.execute_input":"2025-02-04T17:01:09.39096Z","iopub.status.idle":"2025-02-04T17:01:13.559183Z","shell.execute_reply.started":"2025-02-04T17:01:09.390917Z","shell.execute_reply":"2025-02-04T17:01:13.558046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def f(x):\n    tmp = x.copy()\n    preprocess_input(tmp)\n    return model(tmp)\n\n\n# define a masker that is used to mask out partitions of the input image.\nmasker = shap.maskers.Image(\"inpaint_telea\", X[0].shape)\n\n# create an explainer with model and image masker\nexplainer = shap.Explainer(f, masker, output_names=class_names)\n\n# here we explain two images using 500 evaluations of the underlying model to estimate the SHAP values\nshap_values = explainer(X[1:3], max_evals=1000, batch_size=50, outputs=shap.Explanation.argsort.flip[:4])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:01:13.561138Z","iopub.execute_input":"2025-02-04T17:01:13.561507Z","iopub.status.idle":"2025-02-04T17:02:41.624512Z","shell.execute_reply.started":"2025-02-04T17:01:13.561479Z","shell.execute_reply":"2025-02-04T17:02:41.623261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shap.image_plot(shap_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T17:02:42.727997Z","iopub.execute_input":"2025-02-04T17:02:42.728306Z","iopub.status.idle":"2025-02-04T17:02:43.736322Z","shell.execute_reply.started":"2025-02-04T17:02:42.72828Z","shell.execute_reply":"2025-02-04T17:02:43.7354Z"}},"outputs":[],"execution_count":null}]}